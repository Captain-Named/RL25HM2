"""Utility functions for PPO TicTacToe training.

Categories:
1. Core Utilities
2. Training Metrics & Visualization 
3. Board Operations
4. Debug & Testing
"""

import datetime
import torch
import numpy as np
from torch.nn import functional as F      
import json
import matplotlib.pyplot as plt
import os

# 1. Core Utility Functions
#Note that this funciton _calculate_state is used in the network.py
def _calculate_state(x: torch.tensor):
    """Calculate potential scores for each valid position.
    
    Args:
        x: Input tensor of shape [batch_size, 2, 12, 12]
           Channel 0: Current player positions
           Channel 1: Opponent positions
           
    Returns:
        torch.Tensor: Potential scores mask of shape [batch_size, 80]
    """
    batch_size = x.shape[0]
    mask = torch.zeros((x.shape[0],80), dtype=torch.float32, device=x.device)
    
    for b in range(batch_size):
        # Extract board state for current batch
        my_view = x[b, 0].detach().numpy()  
        opponent_view = x[b, 1].detach().numpy()
        
        # Create combined board state (-1: opponent, 0: empty, 1: current player)
        board = np.zeros((12, 12))
        board[my_view == 1] = 1
        board[opponent_view == 1] = -1

        # Calculate potential scores for valid positions
        for i in range(12):
            for j in range(12):
                if board[i][j] == 0 and _is_in_zone(i,j) and _is_adjacent_to_player(board, i,j):
                    my_score = _calculate_potential(board, i,j)
                    index = position_to_index(i, j)
                    mask[b, index] = my_score
    return mask

def _is_in_zone(ni, nj):
    """Check if position is within valid cross-shaped zone.
    
    Args:
        ni, nj: Board coordinates
        
    Returns:
        bool: True if position is valid
    """
    return not((ni<4 and nj<4)or(ni<4 and nj>7)or(ni>7 and nj<4)or(ni>7 and nj>7)) and (0 <= ni < 12 and 0 <= nj < 12)

def _is_adjacent_to_player(board, i, j):
    """Check if position is adjacent to current player's pieces.
    
    Args:
        board: Game board state
        i, j: Position coordinates
        
    Returns:
        bool: True if adjacent to player's piece
    """
    for di in [-1, 0, 1]:
        for dj in [-1, 0, 1]:
            if di == 0 and dj == 0:
                continue
            ni, nj = i + di, j + dj
            if 0 <= ni < 12 and 0 <= nj < 12 and _is_in_zone(ni, nj):
                if board[ni][nj] == 1:
                    return True
    return False

def _calculate_potential(board, i, j):
    """Calculate potential score for placing piece at position.
    
    Args:
        board: Current board state
        i, j: Position coordinates
        
    Returns:
        float: Potential score based on:
            - Two consecutive: +1.0
            - Three consecutive: +3.0
            - Four consecutive: +10.0
    """
    if not _is_in_zone(i, j) or board[i][j] != 0:
        return 0.0
    
    virtual_board = np.copy(board)
    
    # Calculate score before move
    two_len = _detect_consecutive_vir(1, 2, virtual_board)
    three_len = _detect_consecutive_vir(1, 3, virtual_board) 
    four_len = _detect_consecutive_vir(1, 4, virtual_board)
    score_before = 1.0 * two_len + 3.0 * three_len + 10.0 * four_len
    
    # Calculate score after move
    virtual_board[i][j] = 1
    two_len = _detect_consecutive_vir(1, 2, virtual_board)
    three_len = _detect_consecutive_vir(1, 3, virtual_board) 
    four_len = _detect_consecutive_vir(1, 4, virtual_board)
    score_after = 1.0 * two_len + 3.0 * three_len + 10.0 * four_len
    
    return score_after - score_before

def _detect_consecutive_vir(player, length, vir_board):
    """Count consecutive patterns of given length.
    
    Args:
        player: Player ID to check for
        length: Pattern length to find
        vir_board: Board state to analyze
        
    Returns:
        int: Number of patterns found in:
            - Horizontal lines
            - Vertical lines
            - Main diagonals
            - Counter diagonals
    """
    count = 0
    
    # Check horizontal patterns
    for i in range(12):
        for j in range(12 - length + 1):
            if all(vir_board[i][j+k] == player for k in range(length)):
                count += 1
    
    # Check vertical patterns
    for j in range(12):
        for i in range(12 - length + 1):
            if all(vir_board[i+k][j] == player for k in range(length)):
                count += 1
    
    # Check main diagonals
    for i in range(12 - length + 1):
        for j in range(12 - length + 1):
            if all(vir_board[i+k][j+k] == player for k in range(length)):
                count += 1
    
    # Check counter diagonals
    for i in range(length-1, 12):
        for j in range(12 - length + 1):
            if all(vir_board[i-k][j+k] == player for k in range(length)):
                count += 1
                
    return count

def position_to_index(i: int, j: int) -> int:
    """
    Convert board position (i,j) to action index (0-79).
    
    Args:
        i: row index (0-11)
        j: column index (0-11)
    Returns:
        int: action index (0-79) or None if invalid position
    """
    # Validate input range
    if not (0 <= i < 12 and 0 <= j < 12):
        return None
        
    # Top board (rows 0-3, cols 4-7)
    if 0 <= i < 4 and 4 <= j < 8:
        return i * 4 + (j - 4)
        
    # Left board (rows 4-7, cols 0-3)
    elif 4 <= i < 8 and 0 <= j < 4:
        return 16 + ((i - 4) * 4 + j)
        
    # Center board (rows 4-7, cols 4-7)
    elif 4 <= i < 8 and 4 <= j < 8:
        return 32 + ((i - 4) * 4 + (j - 4))
        
    # Right board (rows 4-7, cols 8-11)
    elif 4 <= i < 8 and 8 <= j < 12:
        return 48 + ((i - 4) * 4 + (j - 8))
        
    # Bottom board (rows 8-11, cols 4-7)
    elif 8 <= i < 12 and 4 <= j < 8:
        return 64 + ((i - 8) * 4 + (j - 4))
        
    # Invalid position
    return None


# 2. Training Metrics & Visualization

def plot_training_metrics(log_file):
    """Plot training metrics from log file.
    
    Args:
        log_file: Path to JSON file containing training metrics
        
    Plots:
        1. Training Losses:
            - Total loss
            - Policy loss 
            - Value loss
        2. Training Rewards:
            - Average reward per batch
            
    Saves:
        PNG file with plots in same directory as log file
    """
    # Load metrics from JSON file
    with open(log_file, 'r') as f:
        metrics = json.load(f)
    
    # Create figure with multiple subplots
    plt.figure(figsize=(15, 5))
    
    # Plot various loss curves
    plt.subplot(131)
    plt.plot(metrics['epoch_losses'], label='Total Loss')
    plt.plot(metrics['policy_losses'], label='Policy Loss')
    plt.plot(metrics['value_losses'], label='Value Loss')
    plt.xlabel('Batch')
    plt.ylabel('Loss')
    plt.title('Training Losses')
    plt.legend()
    plt.grid(True)
    
    # Plot reward curve
    plt.subplot(132)
    plt.plot(metrics['rewards'], label='Average Reward')
    plt.xlabel('Batch')
    plt.ylabel('Reward')
    plt.title('Training Rewards')
    plt.legend()
    plt.grid(True)
    
    # Adjust layout and save figure
    plt.tight_layout()
    plt.savefig(os.path.join(os.path.dirname(log_file), 'training_curves.png'))

def plot_smoothed_loss(losses, window_size=10, save_dir="./training_model", filename="training_loss_curve_avg.png"):
    """ç»˜åˆ¶å¹³æ»‘å’ŒåŸå§‹æŸå¤±æ›²çº¿
    Args:
        losses: æŸå¤±å€¼åˆ—è¡¨
        window_size: å¹³æ»‘çª—å£å¤§å°
        save_dir: ä¿å­˜ç›®å½•
        filename: ä¿å­˜æ–‡ä»¶å
    """
    weights = np.ones(window_size) / window_size
    smoothed_losses = np.convolve(losses, weights, mode='valid')
    
    plt.figure(figsize=(12, 6))
    plt.plot(losses, 'lightgray', alpha=0.3, label='åŸå§‹æŸå¤±')
    plt.plot(range(window_size-1, len(losses)), 
             smoothed_losses, 
             'b-', 
             linewidth=2, 
             label=f'å¹³æ»‘æŸå¤±(çª—å£={window_size})')
    
    plt.xlabel("è®­ç»ƒæ‰¹æ¬¡")
    plt.ylabel("æŸå¤±å€¼")
    plt.title("è®­ç»ƒæŸå¤±æ›²çº¿")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    if save_dir:
        save_path = os.path.join(save_dir, filename)
        plt.savefig(save_path)
    plt.close()


# 3. Debug & Testing Functions 

def print_reset_td(td, env=None, title="env.reset()è¿”å›çš„TensorDictä¿¡æ¯", prefix="  "):
    """
    æ‰“å°ç¯å¢ƒreset()è¿”å›çš„TensorDictä¿¡æ¯ï¼Œæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„å½¢å¼
    
    reset()è¿”å›çš„tdå½¢å¦‚ä¸‹ï¼š

    TensorDict(
        fields={
            action_mask: Tensor(shape=torch.Size([9]), device=cuda:0, dtype=torch.bool, is_shared=True),
            done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),
            observation: Tensor(shape=torch.Size([1, 3, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),      
            step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},
        batch_size=torch.Size([]),
        device=None,
        is_shared=False)

    å‚æ•°:
        td: è¦æ‰“å°çš„TensorDict
        env: å¯é€‰çš„ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºæ‰“å°é¢å¤–çš„ç¯å¢ƒçŠ¶æ€
        title: æ‰“å°ä¿¡æ¯çš„æ ‡é¢˜
        prefix: æ‰“å°æ—¶çš„ç¼©è¿›å‰ç¼€
    """
    print(f"\n{'='*40}")
    print(f"ğŸ”„ {title}")
    print(f"{'='*40}")
    
    # æ‰“å°æ£‹ç›˜çŠ¶æ€ (observation)
    if "observation" in td:
        print(f"{prefix}ğŸ‘€ è§‚å¯Ÿå‘é‡ (observation) / resetè¿‡åï¼Œæ£‹ç›˜çŠ¶æ€:")
        obs = td["observation"].cpu().numpy()
        
        # æå–Xå’ŒOçš„ä½ç½®
        board_display = np.full((3,3), '.')
        board_x = np.argwhere(obs[0] > 0)
        board_o = np.argwhere(obs[0] < 0)
        
        for pos in board_x:
            board_display[tuple(pos)] = 'X'
        for pos in board_o:
            board_display[tuple(pos)] = 'O'
        
        # æ‰“å°æ£‹ç›˜
        for row in board_display:
            print(f"{prefix}   |" + "|".join(row) + "|")
        
        current_player = 'X'
        print(f"{prefix}   å½“å‰ç©å®¶: {current_player}")
    
    # æ‰“å°åŠ¨ä½œæ©ç 
    if "action_mask" in td:
        print(f"\n{prefix}ğŸ¯ åŠ¨ä½œæ©ç  (action_mask) / resetè¿‡åçŠ¶æ€çš„åˆæ³•åŠ¨ä½œ:")
        mask = td["action_mask"].cpu().bool()
        
        # æ ¼å¼åŒ–ä¸º3x3æ£‹ç›˜å½¢å¼
        mask_array = mask.reshape(3, 3)
        print(f"{prefix}   ä½ç½®æ ¼å¼: [ä½ç½®ç´¢å¼•:å¯ç”¨æ€§]")
        
        for i in range(3):
            row_str = "   "
            for j in range(3):
                symbol = "âœ“" if mask_array[i, j] else "âœ—"
                row_str += f"[{i*3+j}:{symbol}] "
            print(f"{prefix}{row_str}")
    
    # æ‰“å°æ­¥æ•°è®¡æ•°
    if "step_count" in td:
        step_count = td["step_count"].item()
        print(f"\n{prefix}ğŸ”¢ æ­¥æ•° (step_count) / resetè¿‡åï¼Œstepç½®0: {step_count}")
    
    # æ‰“å°doneçŠ¶æ€
    if "done" in td:
        done = td["done"].item()
        print(f"{prefix}ğŸ æ˜¯å¦ç»“æŸ (done) / resetè¿‡åçš„çŠ¶æ€æ˜¯å¦æ˜¯ç»ˆæ­¢çŠ¶æ€: {'æ˜¯' if done else 'å¦'}")
    
    # æ‰“å°terminatedçŠ¶æ€
    if "terminated" in td:
        terminated = td["terminated"].item()
        print(f"{prefix}âš ï¸ æ˜¯å¦ç»ˆæ­¢ (terminated) / resetè¿‡åçš„çŠ¶æ€æ˜¯å¦æ˜¯ç»ˆæ­¢çŠ¶æ€: {'æ˜¯' if terminated else 'å¦'}")
    
    # å¦‚æœæä¾›äº†ç¯å¢ƒï¼Œæ‰“å°é¢å¤–çš„ç¯å¢ƒçŠ¶æ€
    if env is not None:
        print(f"\n{prefix}ğŸŒ resetè¿‡åï¼Œenv.boardçš„ç¯å¢ƒçŠ¶æ€:")
        if hasattr(env, "render_board"):
            board_str = env.render_board()
            for line in board_str.strip().split('\n'):
                print(f"{prefix}   {line}")
        
        if hasattr(env, "current_player"):
            player_symbol = 'X' if env.current_player == 1 else 'O'
            print(f"{prefix}   å½“å‰ç©å®¶: {player_symbol}")
        
        if hasattr(env, "winner"):
            winner = env.winner
            winner_str = "æ— " if winner == 0 else ("X" if winner == 1 else "O")
            print(f"{prefix}   å½“å‰èµ¢å®¶: {winner_str}")
    
    print(f"{'='*40}\n")

def print_step_td(td, env=None, title="step(policy_output)åè¿”å›çš„TensorDictä¿¡æ¯", prefix="  ", max_tensor_items=30):
    """
    æ‰“å°stepè¿”å›çš„tdçš„è¯¦ç»†ä¿¡æ¯ï¼Œæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„å½¢å¼
    
    step(policy_ouput)è¿”å›çš„tdå½¢å¦‚ä¸‹ï¼š

    step(policy_ouput): TensorDict(
        fields={
            action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),
            action_mask: Tensor(shape=torch.Size([9]), device=cuda:0, dtype=torch.bool, is_shared=True),
            done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),
            logits: Tensor(shape=torch.Size([1, 9]), device=cuda:0, dtype=torch.float32, is_shared=True),
            next: TensorDict(
                fields={
                    action_mask: Tensor(shape=torch.Size([9]), device=cuda:0, dtype=torch.bool, is_shared=True),       
                    done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),
                    observation: Tensor(shape=torch.Size([3, 3, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),
                    reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
                    step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float64, is_shared=True),     
                    terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},       
                batch_size=torch.Size([]),
                device=None,
                is_shared=False),
            observation: Tensor(shape=torch.Size([3, 3, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),      
            sample_log_prob: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),        
            state_value: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},
        batch_size=torch.Size([]),
        device=None,
        is_shared=False)

    å‚æ•°:
        td: è¦æ‰“å°çš„TensorDict
        env: å¯é€‰çš„ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºæ‰“å°æ£‹ç›˜çŠ¶æ€
        title: æ‰“å°ä¿¡æ¯çš„æ ‡é¢˜
        prefix: æ‰“å°æ—¶çš„ç¼©è¿›å‰ç¼€
        max_tensor_items: æ˜¾ç¤ºtensorçš„æœ€å¤§å…ƒç´ æ•°é‡
    """
    def tensor_to_readable(tensor, max_items=max_tensor_items):
        """å°†å¼ é‡è½¬æ¢ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
        if tensor.numel() <= max_items:
            return tensor.cpu().tolist()
        else:
            flat = tensor.flatten()
            return "[å‰{}ä¸ª: {}, ... å½¢çŠ¶: {}]".format(
                max_items,
                flat[:max_items].cpu().tolist(),
                tensor.shape
            )
    
    def print_dict_items(d, level=0, path=""):
        """é€’å½’æ‰“å°å­—å…¸é¡¹"""
        indent = prefix * (level + 1)
        if isinstance(d, dict) or hasattr(d, "keys"):
            for key in d.keys():
                current_path = f"{path}.{key}" if path else key
                if key == "next" and isinstance(d[key], dict):
                    print(f"{indent}ğŸ”„ {key}: (nextçŠ¶æ€)")
                    print_dict_items(d[key], level + 1, current_path)
                elif isinstance(d[key], dict) or hasattr(d[key], "keys"):
                    print(f"{indent}ğŸ“‚ {key}: (åµŒå¥—å­—å…¸)")
                    print_dict_items(d[key], level + 1, current_path)
                elif torch.is_tensor(d[key]):
                    if key == "observation" and d[key].shape[-3:] == (3, 3, 3) and level == 0:
                        print(f"{indent}ğŸ‘€ {key} (stepä¹‹å‰çš„è§‚å¯Ÿå‘é‡): ")
                        if d[key].dim() > 3:
                            # å¦‚æœæ˜¯batchå½¢å¼
                            sample_obs = d[key][0]
                        else:
                            sample_obs = d[key]
                        
                        obs_np = sample_obs.cpu().numpy()
                        # æ£‹ç›˜è¡¨ç¤º
                        board_display = np.full((3,3), '.')
                        board_x = np.argwhere(obs_np[0] > 0)
                        board_o = np.argwhere(obs_np[0] < 0)
                        for pos in board_x:
                            board_display[tuple(pos)] = 'X'
                        for pos in board_o:
                            board_display[tuple(pos)] = 'O'
                        print(f"{indent}   è§‚å¯Ÿå‘é‡: {tensor_to_readable(sample_obs)}")
                        print(f"{indent}   æ£‹ç›˜çŠ¶æ€:")
                        for row in board_display:
                            print(f"{indent}   |" + "|".join(row) + "|")
                        
                        current_player = 'X'
                        print(f"{indent}   å½“å‰ç©å®¶: {current_player}")
                    elif key == "action_mask":
                        print(f"{indent}ğŸ¯ {key} (åŠ¨ä½œæ©ç ): ")
                        mask = d[key].cpu().bool()
                        if mask.dim() == 1 and mask.shape[0] == 9:
                            # 3x3æ¿
                            mask_array = mask.reshape(3, 3)
                            for i in range(3):
                                row_str = "   "
                                for j in range(3):
                                    symbol = "âœ“" if mask_array[i, j] else "âœ—"
                                    row_str += f"[{i*3+j}:{symbol}] "
                                print(f"{indent}{row_str}")
                        else:
                            print(f"{indent}   {tensor_to_readable(mask)}")
                    elif key == "action":
                        action_val = d[key].item()
                        row, col = divmod(action_val, 3)
                        print(f"{indent}ğŸ® {key} (åŠ¨ä½œ): {action_val} (ä½ç½®: {row},{col})")
                    elif key == "reward":
                        reward_val = d[key].item()
                        print(f"{indent}ğŸ† {key} (å¥–åŠ±): {reward_val:.4f}")
                    elif key == "state_value":
                        value = d[key].item()
                        print(f"{indent}ğŸ’° {key} (çŠ¶æ€ä»·å€¼): {value:.4f}")
                    elif key == "logits":
                        print(f"{indent}ğŸ§  {key} (ç­–ç•¥logits):")
                        logits = d[key].squeeze(0).cpu()
                        if logits.shape[0] == 9:
                            # æ ¼å¼åŒ–ä¸º3x3çš„ç½‘æ ¼
                            logits_array = logits.reshape(3, 3)
                            for i in range(3):
                                row_str = "   "
                                for j in range(3):
                                    row_str += f"[{logits_array[i, j]:.4f}] "
                                print(f"{indent}{row_str}")
                            
                            # è®¡ç®—softmaxæ¦‚ç‡
                            probs = F.softmax(logits, dim=0)
                            print(f"{indent}   ï¼ˆå»æ‰éæ³•åŠ¨ä½œå‰çš„ï¼‰æ¦‚ç‡åˆ†å¸ƒ:")
                            probs_array = probs.reshape(3, 3)
                            for i in range(3):
                                row_str = "   "
                                for j in range(3):
                                    row_str += f"[{probs_array[i, j]:.4f}] "
                                print(f"{indent}{row_str}")
                        else:
                            print(f"{indent}   {tensor_to_readable(logits)}")
                    elif key == "sample_log_prob":
                        log_prob = d[key].item()
                        prob = np.exp(log_prob)
                        print(f"{indent}ğŸ“Š {key} (ï¼ˆå»æ‰éæ³•åŠ¨ä½œåçš„ï¼‰åŠ¨ä½œå¯¹æ•°æ¦‚ç‡): {log_prob:.4f} (ï¼ˆå»æ‰éæ³•åŠ¨ä½œåçš„ï¼‰æ¦‚ç‡: {prob:.4%})")
                    elif key == "done" or key == "terminated":
                        print(f"{indent}ğŸ {key}: {'æ˜¯' if d[key].item() else 'å¦'}")
                    elif key == "step_count":
                        print(f"{indent}ğŸ”¢ {key} (æ­¥æ•°): {d[key].item()}")
                    else:
                        print(f"{indent}ğŸ“Š {key}: {tensor_to_readable(d[key])}")
                else:
                    print(f"{indent}ğŸ”§ {key}: {d[key]}")
    
    print(f"\n{'='*40}")
    print(f"ğŸ” {title}")
    print(f"{'='*40}")
    
    # æ‰“å°ä¸»ä½“ä¿¡æ¯
    if isinstance(td, dict) or hasattr(td, "keys"):
        print_dict_items(td)
    else:
        print(f"{prefix}ä¸æ˜¯TensorDictæˆ–å­—å…¸ç±»å‹: {type(td)}")
    
    # å¦‚æœæä¾›äº†ç¯å¢ƒï¼Œæ‰“å°ç¯å¢ƒçŠ¶æ€
    if env is not None:
        print(f"\n{prefix}ğŸŒ æ‰§è¡Œäº†stepåï¼Œenvæ£‹ç›˜çŠ¶æ€:")
        board_str = env.render_board()
        for line in board_str.strip().split('\n'):
            print(f"{prefix}{line}")
        print(f"{prefix}å½“å‰ç©å®¶: {'X' if env.current_player == 1 else 'O'}")
    
    print(f"{'='*40}\n")

def print_actor_output(td, env=None, title="actor_module(td)è¿”å›çš„ç­–ç•¥è¾“å‡º", prefix="  "):
    """
    æ‰“å°ç­–ç•¥ç½‘ç»œ(actor_module)è¾“å‡ºçš„TensorDictä¿¡æ¯ï¼Œæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„å½¢å¼
    
    actor_module(td)è¿”å›çš„tdå½¢å¦‚ä¸‹ï¼š
    
    policy_output: TensorDict(
        fields={
            # è¿™é‡Œæ˜¯æ ¹æ®logitså’Œmaské€‰å‡ºçš„æœ€ç»ˆçš„ä¸€ä¸ªaction           
            action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),
            #è¿™é‡Œçš„action_maskæ˜¯è¾“å…¥çš„tdçš„å…¶ä¸­åŸå°ä¸åŠ¨çš„action_mask
            action_mask: Tensor(shape=torch.Size([9]), device=cuda:0, dtype=torch.bool, is_shared=True),
            #è¿™é‡Œçš„doneæ˜¯åŸæ¥çš„done
            done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),
            #è¾“å…¥policyç½‘ç»œçš„è¾“å‡º
            logits: Tensor(shape=torch.Size([1, 9]), device=cuda:0, dtype=torch.float32, is_shared=True),
            observation: Tensor(shape=torch.Size([1, 3, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),      
            sample_log_prob: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),        
            step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},
        batch_size=torch.Size([]),
        device=None,
        is_shared=False)
    
    å‚æ•°:
        td: è¦æ‰“å°çš„ç­–ç•¥è¾“å‡ºTensorDict
        env: å¯é€‰çš„ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºæ‰“å°æ£‹ç›˜çŠ¶æ€
        title: æ‰“å°ä¿¡æ¯çš„æ ‡é¢˜
        prefix: æ‰“å°æ—¶çš„ç¼©è¿›å‰ç¼€
    """
    print(f"\n{'='*40}")
    print(f"ğŸ² {title}")
    print(f"{'='*40}")
    
    # 1. ç­–ç•¥é€‰æ‹©çš„åŠ¨ä½œ
    if "action" in td:
        action_val = td["action"].item()
        row, col = divmod(action_val, 3)
        print(f"{prefix}ğŸ® é€‰æ‹©çš„åŠ¨ä½œ: {action_val} (ä½ç½®: {row},{col})")
    
    # 2. ç­–ç•¥logitså’Œæ¦‚ç‡åˆ†å¸ƒ
    if "logits" in td:
        print(f"\n{prefix}ğŸ§  ç­–ç•¥logitså€¼:")
        logits = td["logits"].squeeze(0).cpu()
        
        # æ ¼å¼åŒ–ä¸º3x3æ£‹ç›˜
        logits_array = logits.reshape(3, 3)
        for i in range(3):
            row_str = f"{prefix}   "
            for j in range(3):
                row_str += f"[{logits_array[i, j]:.4f}] "
            print(row_str)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
        probs = torch.softmax(logits, dim=0)
        print(f"\n{prefix}ğŸ“Š åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ:")
        prob_array = probs.reshape(3, 3)
        
        # å¦‚æœæœ‰åŠ¨ä½œæ©ç ï¼Œç»“åˆæ˜¾ç¤º
        has_mask = "action_mask" in td
        if has_mask:
            mask = td["action_mask"].cpu().bool()
            mask_array = mask.reshape(3, 3)
        
        for i in range(3):
            row_str = f"{prefix}   "
            for j in range(3):
                pos_idx = i*3 + j
                prob = prob_array[i, j]
                
                if has_mask:
                    symbol = "âœ“" if mask_array[i, j] else "âœ—"
                    row_str += f"[{pos_idx}:{prob:.4f}{symbol}] "
                else:
                    row_str += f"[{pos_idx}:{prob:.4f}] "
            print(row_str)
    
    # 3. åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
    if "sample_log_prob" in td:
        log_prob = td["sample_log_prob"].item()
        prob = np.exp(log_prob)
        print(f"\n{prefix}ğŸ“ é€‰æ‹©åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡: {log_prob:.4f}")
        print(f"{prefix}   å¯¹åº”æ¦‚ç‡å€¼: {prob:.4%}")
    
    # 4. è§‚å¯Ÿå‘é‡ (å½“å‰æ£‹ç›˜çŠ¶æ€)
    if "observation" in td:
        print(f"\n{prefix}ğŸ‘€ å½“å‰è§‚å¯ŸçŠ¶æ€ (æ£‹ç›˜):")
        obs = td["observation"].cpu().numpy()
        
        # æå–Xå’ŒOçš„ä½ç½®
        board_display = np.full((3,3), '.')
        board_x = np.argwhere(obs[0] > 0)
        board_o = np.argwhere(obs[0] < 0)
        
        for pos in board_x:
            board_display[tuple(pos)] = 'X'
        for pos in board_o:
            board_display[tuple(pos)] = 'O'
        
        # æ‰“å°æ£‹ç›˜
        for row in board_display:
            print(f"{prefix}   |" + "|".join(row) + "|")
        
        
        current_player = 'X'
        print(f"{prefix}   å½“å‰ç©å®¶: {current_player}")
    
    # 5. æ¸¸æˆçŠ¶æ€ä¿¡æ¯
    print(f"\n{prefix}ğŸ¯ æ¸¸æˆçŠ¶æ€ä¿¡æ¯:")
    
    if "step_count" in td:
        step_count = td["step_count"].item()
        print(f"{prefix}   æ­¥æ•°: {step_count}")
    
    if "done" in td:
        done = td["done"].item()
        print(f"{prefix}   æ˜¯å¦ç»“æŸ: {'æ˜¯' if done else 'å¦'}")
    
    if "terminated" in td:
        terminated = td["terminated"].item()
        print(f"{prefix}   æ˜¯å¦ç»ˆæ­¢: {'æ˜¯' if terminated else 'å¦'}")
    
    # 6. ç¯å¢ƒçŠ¶æ€æ¯”å¯¹ (å¦‚æœæä¾›)
    if env is not None:
        print(f"\n{prefix}ğŸŒ ç¯å¢ƒä¸­çš„å®é™…çŠ¶æ€:")
        if hasattr(env, "render_board"):
            board_str = env.render_board()
            for line in board_str.strip().split('\n'):
                print(f"{prefix}   {line}")
        
        if hasattr(env, "current_player"):
            player_symbol = 'X' if env.current_player == 1 else 'O'
            print(f"{prefix}   å½“å‰ç©å®¶: {player_symbol}")
    
    # 7. ç­–ç•¥åˆ†æ - æœ€ä¼˜åŠ¨ä½œä¸æ©ç å¯¹æ¯”
    if "logits" in td and "action_mask" in td:
        print(f"\n{prefix}ğŸ” ç­–ç•¥åˆ†æ:")
        mask = td["action_mask"].cpu().bool()
        logits = td["logits"].squeeze(0).cpu()
        probs = torch.softmax(logits, dim=0)
        
        # æ‰¾å‡ºåˆæ³•åŠ¨ä½œä¸­æ¦‚ç‡æœ€é«˜çš„
        masked_probs = probs.clone()
        masked_probs[~mask] = -float('inf')
        best_legal_action = torch.argmax(masked_probs).item()
        best_prob = probs[best_legal_action].item()
        
        # æ‰¾å‡ºæ•´ä½“æ¦‚ç‡æœ€é«˜çš„
        overall_best_action = torch.argmax(probs).item()
        
        # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†æœ€ä¼˜åˆæ³•åŠ¨ä½œ
        chosen_action = td["action"].item() if "action" in td else None
        best_row, best_col = divmod(best_legal_action, 3)
        
        print(f"{prefix}   æœ€ä¼˜åˆæ³•åŠ¨ä½œ: {best_legal_action} (ä½ç½®: {best_row},{best_col}), æ¦‚ç‡: {best_prob:.4f}")
        
        if chosen_action is not None:
            is_optimal = chosen_action == best_legal_action
            print(f"{prefix}   æ‰€é€‰åŠ¨ä½œæ˜¯å¦æœ€ä¼˜: {'æ˜¯' if is_optimal else 'å¦'}")
            
            if not is_optimal:
                chosen_prob = probs[chosen_action].item()
                print(f"{prefix}   æ‰€é€‰åŠ¨ä½œæ¦‚ç‡: {chosen_prob:.4f} vs æœ€ä¼˜åŠ¨ä½œæ¦‚ç‡: {best_prob:.4f}")
        
        if overall_best_action != best_legal_action:
            overall_best_row, overall_best_col = divmod(overall_best_action, 3)
            overall_best_prob = probs[overall_best_action].item()
            print(f"{prefix}   æ³¨æ„: æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ {overall_best_action} (ä½ç½®: {overall_best_row},{overall_best_col}) æ˜¯éæ³•çš„")
    
    print(f"{'='*40}\n")

def print_value_output(td, env=None, title="value_module(td)è¿”å›çš„ä»·å€¼ä¼°è®¡", prefix="  "):
    """
    æ‰“å°ä»·å€¼ç½‘ç»œ(value_module)è¾“å‡ºçš„TensorDictä¿¡æ¯ï¼Œæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„å½¢å¼
    
    value_module(td)è¿”å›çš„tdå½¢å¦‚ä¸‹ï¼š
    
    value_output: TensorDict(
        fields={
            action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True), #ä»actionç½‘ç»œä¸­å¾—åˆ°çš„
            action_mask: Tensor(shape=torch.Size([9]), device=cuda:0, dtype=torch.bool, is_shared=True), #æœ€å¼€å§‹çš„tdé‡Œçš„
            done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True), #æœ€å¼€å§‹çš„tdé‡Œçš„
            logits: Tensor(shape=torch.Size([1, 9]), device=cuda:0, dtype=torch.float32, is_shared=True), #ä»actionç½‘ç»œä¸­å¾—åˆ°çš„
            observation: Tensor(shape=torch.Size([1, 3, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),    #æœ€å¼€å§‹çš„tdé‡Œçš„  
            sample_log_prob: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),        
            state_value: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},
        batch_size=torch.Size([]),
        device=None,
        is_shared=False)
    
    å‚æ•°:
        td: è¦æ‰“å°çš„ä»·å€¼ç½‘ç»œè¾“å‡ºTensorDict
        env: å¯é€‰çš„ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºæ‰“å°æ£‹ç›˜çŠ¶æ€
        title: æ‰“å°ä¿¡æ¯çš„æ ‡é¢˜
        prefix: æ‰“å°æ—¶çš„ç¼©è¿›å‰ç¼€
    """
    print(f"\n{'='*40}")
    print(f"ğŸ’° {title}")
    print(f"{'='*40}")
    
    # 1. ä»·å€¼ä¼°è®¡
    if "state_value" in td:
        value = td["state_value"].item()
        print(f"{prefix}ğŸ’° çŠ¶æ€ä»·å€¼ä¼°è®¡: {value:.4f}")
        
        # æä¾›ä»·å€¼è§£é‡Š
        if value > 0.8:
            print(f"{prefix}   è§£é‡Š: éå¸¸æœ‰åˆ©çš„å±€é¢ï¼Œå¾ˆå¯èƒ½è·èƒœ")
        elif value > 0.5:
            print(f"{prefix}   è§£é‡Š: ç›¸å¯¹æœ‰åˆ©çš„å±€é¢")
        elif value > 0.2:
            print(f"{prefix}   è§£é‡Š: ç•¥å¾®æœ‰åˆ©çš„å±€é¢")
        elif value > -0.2:
            print(f"{prefix}   è§£é‡Š: ä¸­æ€§å±€é¢ï¼Œèƒœè´Ÿéš¾æ–™")
        elif value > -0.5:
            print(f"{prefix}   è§£é‡Š: ç•¥å¾®ä¸åˆ©çš„å±€é¢")
        elif value > -0.8:
            print(f"{prefix}   è§£é‡Š: ç›¸å¯¹ä¸åˆ©çš„å±€é¢")
        else:
            print(f"{prefix}   è§£é‡Š: éå¸¸ä¸åˆ©çš„å±€é¢ï¼Œå¯èƒ½é¢ä¸´å¤±è´¥")
    
    # 2. è§‚å¯Ÿå‘é‡ (å½“å‰æ£‹ç›˜çŠ¶æ€)
    if "observation" in td:
        print(f"\n{prefix}ğŸ‘€ å½“å‰è§‚å¯ŸçŠ¶æ€ (æ£‹ç›˜):")
        obs = td["observation"].cpu().numpy()
        
        # æå–Xå’ŒOçš„ä½ç½®
        board_display = np.full((3,3), '.')
        board_x = np.argwhere(obs[0] > 0)
        board_o = np.argwhere(obs[0] < 0)
        
        for pos in board_x:
            board_display[tuple(pos)] = 'X'
        for pos in board_o:
            board_display[tuple(pos)] = 'O'
        
        # æ‰“å°æ£‹ç›˜
        for row in board_display:
            print(f"{prefix}   |" + "|".join(row) + "|")
        
        
        current_player = 'X'
        print(f"{prefix}   å½“å‰ç©å®¶: {current_player}")
        
        # æ£‹ç›˜åˆ†æ
        empty_count = np.sum(board_display == '.')
        x_count = np.sum(board_display == 'X')
        o_count = np.sum(board_display == 'O')
        print(f"{prefix}   æ£‹ç›˜åˆ†æ: X={x_count}ä¸ª, O={o_count}ä¸ª, ç©º={empty_count}ä¸ª")
    
    # 3. æ¸¸æˆçŠ¶æ€ä¿¡æ¯
    print(f"\n{prefix}ğŸ¯ æ¸¸æˆçŠ¶æ€ä¿¡æ¯:")
    
    if "step_count" in td:
        step_count = td["step_count"].item()
        print(f"{prefix}   æ­¥æ•°: {step_count}")
    
    if "done" in td:
        done = td["done"].item()
        print(f"{prefix}   æ˜¯å¦ç»“æŸ: {'æ˜¯' if done else 'å¦'}")
    
    if "terminated" in td:
        terminated = td["terminated"].item()
        print(f"{prefix}   æ˜¯å¦ç»ˆæ­¢: {'æ˜¯' if terminated else 'å¦'}")
    
    # 4. åŠ¨ä½œä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if "action" in td and "sample_log_prob" in td:
        action_val = td["action"].item()
        row, col = divmod(action_val, 3)
        log_prob = td["sample_log_prob"].item()
        prob = np.exp(log_prob)
        
        print(f"\n{prefix}ğŸ® åŠ¨ä½œä¿¡æ¯:")
        print(f"{prefix}   é€‰æ‹©çš„åŠ¨ä½œ: {action_val} (ä½ç½®: {row},{col})")
        print(f"{prefix}   åŠ¨ä½œå¯¹æ•°æ¦‚ç‡: {log_prob:.4f} (æ¦‚ç‡: {prob:.4%})")
    
    # 5. åŠ¨ä½œæ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
    if "action_mask" in td:
        print(f"\n{prefix}ğŸ¯ åŠ¨ä½œæ©ç :")
        mask = td["action_mask"].cpu().bool()
        mask_array = mask.reshape(3, 3)
        
        for i in range(3):
            row_str = f"{prefix}   "
            for j in range(3):
                pos_idx = i*3 + j
                symbol = "âœ“" if mask_array[i, j] else "âœ—"
                row_str += f"[{pos_idx}:{symbol}] "
            print(row_str)
        
        valid_actions = mask.sum().item()
        print(f"{prefix}   å¯ç”¨åŠ¨ä½œæ•°: {valid_actions}/9")
    
    # 6. ç¯å¢ƒçŠ¶æ€æ¯”å¯¹ (å¦‚æœæä¾›)
    if env is not None:
        print(f"\n{prefix}ğŸŒ ç¯å¢ƒä¸­çš„å®é™…çŠ¶æ€:")
        if hasattr(env, "render_board"):
            board_str = env.render_board()
            for line in board_str.strip().split('\n'):
                print(f"{prefix}   {line}")
        
        if hasattr(env, "current_player"):
            player_symbol = 'X' if env.current_player == 1 else 'O'
            print(f"{prefix}   å½“å‰ç©å®¶: {player_symbol}")
            
        if hasattr(env, "winner"):
            winner = env.winner
            winner_str = "æ— " if winner == 0 else ("X" if winner == 1 else "O")
            print(f"{prefix}   å½“å‰èµ¢å®¶: {winner_str}")
    
    print(f"{'='*40}\n")

def test_env(env, actor_module, value_module):
    """Test environment interaction with policy and value networks.
    
    Args:
        env: TicTacToe environment instance
        actor_module: Policy network module
        value_module: Value network module
        
    Flow:
    1. Reset environment and get initial state
    2. Get policy predictions for state
    3. Get value predictions for state  
    4. Step environment with policy action
    5. Repeat 2-4 until episode ends
    """
    print("============================test start=============================")
    
    # Disable gradient computation for testing
    with torch.no_grad():
        # Reset environment and get initial state
        print("====================[[[[[[[env.reset()]]]]]]]===================") 
        td = env.reset()  # Returns TensorDict with obs, mask, step count, done flags
        print_reset_td(td, env)

        # Get policy network predictions
        print("====================[[[[[[actor_module(td)]]]]]]===================")
        policy_output = actor_module(td)  # Get action probabilities
        print_actor_output(policy_output, env, title="actor_module(td)è¿”å›çš„td")

        # Get value network predictions
        print("====================[[[[[[value_module(td)]]]]]]===================")
        value_output = value_module(td)  # Get state value estimate
        print_value_output(value_output, env, title="value_module(td)è¿”å›çš„td")

        # Take first environment step
        print("====================[[[[[[env.step(policy_output)]]]]]]===================")
        td = env.step(policy_output)  # Execute action and get next state
        print(f"~~~~~~~~~~~~~~action_mask: {td['action_mask']}")
        print(f"~~~~~~~~~~~~~~next.action_mask: {td['next']['action_mask']}")
        td = td['next']  # Get next state info
        print_step_td(td, env)

        # Continue episode until done
        step = 0
        while not td["done"].item():
            step += 1
            
            # Get policy prediction for current state
            print(f"====================[[[[[[next{step} -- actor_module(td)]]]]]]===================")
            next_policy_output = actor_module(td)
            print(f"111111111111111 {next_policy_output['action']}")  # Print selected action
            print(f"222222222222222 {next_policy_output['action_mask']}")  # Print valid moves mask

            # Get value prediction for current state
            print(f"====================[[[[[[next{step} -- value_module(td)]]]]]]===================")
            next_value_output = value_module(td)

            # Take environment step
            print(f"====================[[[[[[next{step} -- env.step(next_policy_output)]]]]]]===================")
            td = env.step(next_value_output)  # Execute action
            print(f"~~~~~~~~~~~~~~action_mask: {td['action_mask']}")
            print(f"~~~~~~~~~~~~~~next.action_mask: {td['next']['action_mask']}")
            td = td['next']  # Get next state
    
    print("============================test end=============================")

def inspect_replay_buffer(buffer, num_samples=3):
    """æ£€æŸ¥å¹¶æ‰“å°å›æ”¾ç¼“å†²åŒºä¸­çš„æ•°æ®"""
    print("\n" + "="*60)
    print("ã€å›æ”¾ç¼“å†²åŒºæ£€æŸ¥ã€‘")
    print(f"ç¼“å†²åŒºå¤§å°: {len(buffer)}")
    
    if len(buffer) == 0:
        print("ç¼“å†²åŒºä¸ºç©º!")
        return
        
    samples = buffer.sample(num_samples)
    print(f"\næŠ½æ ·æŸ¥çœ‹ {num_samples} ä¸ªæ•°æ®ç‚¹:")
    
    # å…ˆæ˜¾ç¤ºé¡¶å±‚é”®
    print("\né¡¶å±‚é”®:")
    for key in samples.keys():
        if isinstance(samples[key], torch.Tensor):
            tensor = samples[key]
            print(f"  {key}: å½¢çŠ¶={tensor.shape}, ç±»å‹={tensor.dtype}")
        else:
            print(f"  {key}: ç±»å‹={type(samples[key])}")
    
    # æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    print("\nç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯:")
    sample = buffer.sample(1)
    for key in sample.keys(recursive=True):
        if key.find('next') == 0:  # åªæŸ¥çœ‹nexté”®ä¸‹çš„å†…å®¹
            tensor = sample[key]
            if isinstance(tensor, torch.Tensor):
                if tensor.numel() < 10:  # å¦‚æœå…ƒç´ æ•°é‡å°‘åˆ™æ‰“å°å…¨éƒ¨
                    print(f"  {key}: {tensor.cpu().tolist()}, å½¢çŠ¶={tensor.shape}")
                else:
                    print(f"  {key}: [...], å½¢çŠ¶={tensor.shape}, æœ€å¤§å€¼={tensor.max().item():.4f}, æœ€å°å€¼={tensor.min().item():.4f}")

    print("="*60 + "\n")


def pretty_print_tensordict(td, title="TensorDict å†…å®¹è¯¦æƒ…", max_items=10, max_samples=None):
    """
    ä»¥äººç±»å¯è¯»çš„æ–¹å¼æ‰“å° TensorDict çš„å†…å®¹ï¼Œæ”¯æŒæ‰“å°å¤šä¸ªæ ·æœ¬
    
    å‚æ•°:
        td: è¦æ‰“å°çš„ TensorDict
        title: æ ‡é¢˜æ–‡å­—
        max_items: æ¯ä¸ªå¼ é‡æœ€å¤šæ˜¾ç¤ºçš„å…ƒç´ æ•°é‡
        max_samples: æœ€å¤šæ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºæ˜¾ç¤ºå…¨éƒ¨
    """
    import numpy as np
    
    def format_tensor(tensor, max_items=max_items):
        """æ ¼å¼åŒ–å¼ é‡ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
        if tensor.numel() <= max_items:
            # æ˜¾ç¤ºå…¨éƒ¨å…ƒç´ 
            content = str(tensor.cpu().tolist())
        else:
            # æ˜¾ç¤ºéƒ¨åˆ†å…ƒç´ å’Œå½¢çŠ¶
            flat = tensor.flatten()
            content = f"[{', '.join([f'{x:.4f}' if isinstance(x.item(), float) else str(x.item()) for x in flat[:max_items]])}...]"
        
        return f"{content} (å½¢çŠ¶: {tensor.shape}, ç±»å‹: {tensor.dtype})"
    
    def format_board(obs):
        """å°†è§‚å¯Ÿå‘é‡æ ¼å¼åŒ–ä¸ºæ£‹ç›˜è¡¨ç¤º"""
        if obs.shape[-3:] != (3, 3, 3):
            return "éæ ‡å‡†æ£‹ç›˜å½¢çŠ¶"
            
        # ç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªå•ä¸€çš„è§‚å¯Ÿï¼Œè€Œä¸æ˜¯æ‰¹æ¬¡
        if obs.dim() > 3:
            obs = obs[0]
            
        obs_np = obs.cpu().numpy()
        board = np.full((3, 3), '.')
        
        for i in range(3):
            for j in range(3):
                if obs_np[0, i, j] > 0:
                    board[i, j] = 'X'
                elif obs_np[1, i, j] > 0:
                    board[i, j] = 'O'
        
        player = "X" if obs_np[2, 0, 0] > 0 else "O"
        
        board_str = f"å½“å‰ç©å®¶: {player}\n"
        for row in board:
            board_str += "|" + "|".join(row) + "|\n"
        return board_str
    
    def format_action_mask(mask):
        """å°†åŠ¨ä½œæ©ç æ ¼å¼åŒ–ä¸ºæ˜“è¯»å½¢å¼"""
        if mask.shape[-1] != 9:
            return format_tensor(mask)
            
        if mask.dim() > 1:
            mask = mask[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
            
        mask_np = mask.cpu().numpy().reshape(3, 3)
        result = "å¯ç”¨ä½ç½® (âˆš=å¯ç”¨, Ã—=ä¸å¯ç”¨):\n"
        
        for i in range(3):
            row = "|"
            for j in range(3):
                pos = i * 3 + j
                symbol = "âˆš" if mask_np[i, j] else "Ã—"
                row += f"{pos}:{symbol}|"
            result += row + "\n"
            
        return result
    
    def print_tensordict(td, indent=0):
        """é€’å½’æ‰“å° TensorDict çš„å†…å®¹"""
        prefix = "  " * indent
        
        # å¤„ç†ç©º TensorDict
        if td.is_empty():
            print(f"{prefix}(ç©º TensorDict)")
            return
            
        for key in td.keys():
            value = td[key]
            
            if key == "observation":
                print(f"{prefix}ğŸ® {key}:\n{prefix}{format_board(value)}")
            elif key == "action_mask":
                print(f"{prefix}ğŸ¯ {key}:\n{prefix}{format_action_mask(value)}")
            elif key == "action":
                # å°†åŠ¨ä½œè½¬æ¢ä¸ºåæ ‡
                if value.dim() > 1:
                    action_val = value[0].item()  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                else:
                    action_val = value.item()
                row, col = divmod(action_val, 3)
                print(f"{prefix}ğŸ‘† {key}: {action_val} (ä½ç½®: {row},{col})")
            elif key == "reward":
                if value.dim() > 1:
                    reward_val = value[0].item()  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                else:
                    reward_val = value.item()
                print(f"{prefix}ğŸ† {key}: {reward_val:.4f}")
            elif key == "done" or key == "terminated":
                if value.dim() > 1:
                    done_val = value[0].item()  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                else:
                    done_val = value.item()
                print(f"{prefix}ğŸ {key}: {'æ˜¯' if done_val else 'å¦'}")
            elif isinstance(value, dict) or hasattr(value, "keys"):
                # é‡åˆ°åµŒå¥—çš„ TensorDict
                print(f"{prefix}ğŸ“‚ {key}:")
                print_tensordict(value, indent + 1)
            elif hasattr(value, "shape"):
                # å…¶ä»–å¼ é‡
                print(f"{prefix}ğŸ“Š {key}: {format_tensor(value)}")
            else:
                # éå¼ é‡å€¼
                print(f"{prefix}ğŸ”– {key}: {value}")
    
    print("\n" + "="*60)
    print(f"ğŸ” {title}")
    print("="*60)
    
    print(f"æ‰¹æ¬¡å¤§å°: {td.batch_size}")
    print(f"è®¾å¤‡: {td.device}\n")
    
    # å¦‚æœæ‰¹æ¬¡å¤§å°éç©ºï¼Œæ‰“å°æ‰€æœ‰æ ·æœ¬çš„å†…å®¹
    if td.batch_size and td.batch_size[0] > 0:
        num_samples = td.batch_size[0]
        samples_to_print = num_samples if max_samples is None else min(num_samples, max_samples)
        print(f"ã€æ˜¾ç¤º {samples_to_print}/{num_samples} ä¸ªæ ·æœ¬çš„ä¿¡æ¯ã€‘\n")
        
        for sample_idx in range(samples_to_print):
            print(f"\n=== æ ·æœ¬ {sample_idx+1}/{samples_to_print} ===")
            
            # æ‰“å°æ ·æœ¬è§‚å¯Ÿ (æ£‹ç›˜çŠ¶æ€)
            if "observation" in td:
                print("æ£‹ç›˜çŠ¶æ€:")
                obs = td["observation"][sample_idx]
                obs_np = obs.cpu().numpy()
                board_display = np.full((3,3), '.')
                
                for i in range(3):
                    for j in range(3):
                        if obs_np[0, i, j] > 0:
                            board_display[i, j] = 'X'
                        elif obs_np[1, i, j] > 0:
                            board_display[i, j] = 'O'
                
                for row in board_display:
                    print("|" + "|".join(row) + "|")
                
                player_channel = obs_np[2, 0, 0]
                current_player = 'X' if player_channel > 0 else 'O'
                print(f"å½“å‰ç©å®¶: {current_player}")
            
            # æ‰“å°åŠ¨ä½œæ©ç 
            if "action_mask" in td:
                print("\nåŠ¨ä½œæ©ç :")
                mask = td["action_mask"][sample_idx].reshape(3, 3).cpu().numpy()
                for i in range(3):
                    row = "|"
                    for j in range(3):
                        pos = i * 3 + j
                        symbol = "âˆš" if mask[i, j] else "Ã—"
                        row += f"{pos}:{symbol}|"
                    print(row)
            
            # æ‰“å°åŠ¨ä½œ
            if "action" in td:
                action = td["action"][sample_idx].item()
                row, col = divmod(action, 3)
                print(f"\næ‰§è¡ŒåŠ¨ä½œ: {action} (ä½ç½®: {row},{col})")
            
            # æ‰“å°åŠ¨ä½œæ¦‚ç‡
            if "sample_log_prob" in td:
                log_prob = td["sample_log_prob"][sample_idx].item()
                prob = np.exp(log_prob)
                print(f"åŠ¨ä½œæ¦‚ç‡: {prob:.4%} (log prob: {log_prob:.4f})")
            
            # æ‰“å°å¥–åŠ±
            if "next" in td and "reward" in td["next"]:
                reward = td["next", "reward"][sample_idx].item()
                print(f"è·å¾—å¥–åŠ±: {reward:.4f}")
            
            # æ‰“å°æ¸¸æˆæ˜¯å¦ç»“æŸ
            if "next" in td and "done" in td["next"]:
                done = td["next", "done"][sample_idx].item()
                print(f"æ¸¸æˆç»“æŸ: {'æ˜¯' if done else 'å¦'}")
    
    # æ‰“å°å®Œæ•´ç»“æ„
    print("\nå®Œæ•´ TensorDict ç»“æ„:")
    print_tensordict(td)
    print("="*60 + "\n")


def setup_logging(model_dir):
    """Set up logging directory and file for training metrics.
    
    Args:
        model_dir: Base directory for model files
        
    Returns:
        str: Path to JSON log file for training metrics
    """
    # Create logs subdirectory
    log_dir = os.path.join(model_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)
    
    # Create log file path
    log_file = os.path.join(log_dir, f'training.json')
    return log_file

def log_metrics_function(log_file, metrics):
    """Save training metrics to JSON file.
    
    Args:
        log_file: Path to JSON log file
        metrics: Dictionary containing training metrics:
            - epoch_losses: List of average losses per epoch
            - policy_losses: List of policy losses
            - value_losses: List of value network losses
            - entropy_losses: List of entropy losses
            - rewards: List of average rewards
            - lr: List of learning rates
    """
    with open(log_file, 'w') as f:
        json.dump(metrics, f, indent=4)


